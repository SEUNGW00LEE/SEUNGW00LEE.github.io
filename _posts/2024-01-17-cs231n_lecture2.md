---
layout: single
title: "[CS231n review] Lecture2 - Image Classification pipeline"
categories: [AI, Computer Vision, CS231n]
tag: [CS231n, Computer Vision, AI, Image Classification]
typora-root-url: ../
use_math: true
---

<br><font color=gray>Stanford CS231n(2017)를 학습하며 정리 및 추가한 내용입니다.</font> <br>

<br>

# Image Classification

![png](https://www.tensorflow.org/static/tutorials/images/classification_files/output_wBmEA9c0JYes_0.png?hl=ko)



## **Image Classification?**

- 컴퓨터 비전의 주요 문제로서 물체를 인식하고 이미지에 해당하는 label 값을 예측하는 것입니다.



### **Problem : Semantic Gap**<br>

**Semantic Gap?**





- The difficulty of determining a set of image features that correspond to a certain semantic meaning.

- 고양이라는 category는 '사람'이 이미지에 붙인 **"의미상의" 레이블**입니다. 고양이 사진이라는 사실과 실제 '컴퓨터'가 보는 픽셀값과는 큰 차이가 존재하는데,
  이를 바로 **Semantic Gap**이라고 부르는 것입니다.

<br>

**Challenge** 



- Image Classification에 문제가 되는 요소는 다양한데, 아래와 같습니다.

  > 1. Viewpoint variation : viewpoint가 변할 때마다, 모든 픽셀은 변하게 됨
  > 2. Illumination 
  > 3. Deformation : object 형태의 변형
  > 4. Occlusion : 구조물 등으로 object의 일부만 주어질 때
  > 5. Background Clutter : object와 배경이 비슷한 색으로 주어질 때
  > 6. Interclass variation : 같은 카테고리에서의 구분

<br>

## **Solution**



- 위의 문제를 아래의 **<u>데이터 기반 접근</u>**(**<u>Data-Driven Approach</u>**)를 통해 해결합니다.
  - [data - label] collect
  - 머신러닝을 통한 classifier train
  - classifer 평가

<br>

**<mark style='background-color: #f1f8ff'> [Solution - 1] Nearest Neighbor</mark>**



- 가장 가까운 거리에 있는 것을 분류하는 classifier입니다.

> Distance Metric : 어떤 것이 가까운 거리를 의미하는 지를 판단하기 위한 비교수단
>
> \- L1 (Manhattan) distance  : $d_{1}(I_{1},I_{2}) = \sum_{p} \left\lvert I^p_{1} - I^p_{2} \right\rvert$
>
> - tend to follow the coordinate axes
>
> \- L2 (Euclidean) distance  : $d_{2}(I_{1},I_{2}) = \sqrt {\sum_{p}  (I^p_{1} - I^p_{2})^2}$
>
> - doesn't care coordinate axis

- **Problem** : test time performance 는 train time performance보다 중요합니다. <br>&nbsp;&nbsp;&nbsp;그러나 Nearest Neighbor은 predict time이 **linearly 하게 증가**한다는 문제를 가집니다.
  - CNNs은 이 문제를 해결합니다. <br> CNNs는 expensive training하지만 **cheap test evaluation**한 장점을 가집니다.

<br>

**<mark style='background-color: #f1f8ff'>[Solution - 2] K-Nearest Neighbors </mark>**

- K개의 가까운 이웃 중에서 선출(votes)하여 분류하는 classifier입니다.

- **Hyperparameter** --> problem-dependent

  - **distance metrics** - L1 or L2

  - **K**

    K=1 은 training data만을 고려합니다 

    **problem : over-fitting**

    **solution** : split data into train, **val**, test

    - Cross-Validation : Split data into folds > small datasetd에 유용(딥러닝에서 자주 쓰이는 방법은 아님)

- **Problem** 

  1. Test time이 너무 느림

     test data와 비교할 수 있도록 모든 train data들을 기억하고 있어야해서

  2. distance metrics의 오류 문제

  3. 차원의 저주 : 차원이 증가함에 따라 필요한 학습데이터의 양이 기하급수적으로 증가
     <br>
  
     **<font color = 'red'>위의 문제들로 kNN은 이미지 분류에 사용하지않습니다.</font>**
     <br>

**<mark style='background-color: #f1f8ff'>[Solution-3] Linear Classification</mark>** 

- 선형분류는 선형 모델을 이용하여 클래스들로 분류하는 머신러닝 기법입니다.

- **Parametric Approach : Linear Classifier**

  - Linear Classification은 $ f(x,W) = Wx + b $ 로 표현할 수 있는데 $W$는 parameter, $b$ 는 bias입니다.

- **Problem**

  1. color dependence

  ex - Yellow car가 color때문에 frog로 예측
  
  2. 비선형 모델링의 어려움
  3. Optimazation의 어려움
     parameter를 스스로 update하지 못함

<br>

---



#### *<u>Reference</u>*



- [Stanford CS231n](http://cs231n.stanford.edu/)

- [DSBA Lap semina](http://dsba.korea.ac.kr/seminar/?mod=document&uid=17)

- [What is Semantic Gap](https://www.igi-global.com/dictionary/semantic-gap/26326)
- [Tensorflow Core - Image classification](https://www.tensorflow.org/tutorials/images/classification?hl=ko)
