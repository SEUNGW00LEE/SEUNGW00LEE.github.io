---
layout: single
title: "[Paper Review] Deep Residual Learning for Image Recognition"
categories: [AI, Paper Review]
tag: []
typora-root-url: ../
use_math: true
---

<font color=gray>Residual Learning, Shortcut connection을 제시한, ResNet 논문 리뷰입니다. </font> <br>

> 논문 list는 이 [repo](https://github.com/SEUNGW00LEE/paper_review)에 정리되어있습니다.

# [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

---

**K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, Las Vegas, NV, USA, 2016, pp.**

---

**본 논문이 Image Recognition에 큰 영향(인용수 20만 이상)을 끼치는 이유는 크게 2가지가 있다.**

1. **ResNet의 성능이 매우 좋다.**

2. **논문에서 제안한 아이디어가 간단하고 이해하기 쉽다.**



**<font color = 'red'>Main Idea : 깊은 네트워크를 학습시키기 위한 방법으로 residual learning을 제안한다.</font>** 



## **Overview**

**지금까지의 어떤 문제가 있었나?**

- depth가 깊이가 모델의 성능에 큰 영향을 끼치는 것은 자명하지만, 이전의 framework는 network가 깊어질수록 훈련시키기 어려운 문제를 가진다. (오버피팅, gradient 소멸, 연산량 증가 등의 문제)

**해당 논문에서는 어떻게 해결할 수 있나?**

- 함수를 새로 만드는 방법 대신에 residual function(잔차 함수)을 통해 잔차 함수를 learning에 사용하는 것으로 layer를 재구성한다. 
- Residual? : 결과의 오류라고 할 수 있는데, 지금까지 평가의 기준으로 삼았지만 이를 이용해 학습하였다.
- 이를 통한 학습은 학습하기 더 쉽고 이전의 모델보다 훨씬 깊은 layer(152 layer)를 가질 수 있다.

**해당 연구를 통한 성과는 무엇이 있나?**

1. ResNet은 VGG net보다 8배 깊은 152 layer를 가지지만 복잡하지 않고 최적화시키기 쉽고, 깊은 깊이에서도 정확도를 얻을 수 있다.
2. 여기에 앙상블 기법을 적용하여 2015년 ILVRC에서 우승(3.57% error)를 가진다.
3. Image classification 뿐만 아니라, object detection등 전분야에서 탁월한 성능을 보인다.



## **Introduction**

지금까지의 연구를 통해 깊은 ConvNet은 이미지 분류에 있어 중요한 요소이며, 좋은 결과를 위해서는 많은 layer가 필요하다는 것을 알 수 있다.

이전의 VGG-Net이나, GoogLeNet 또한 layer가 깊어짐에 따라 얻는 장점을 적절히 취했다.

필자는 아래와 같은 질문을 남긴다.

**<font color='red'> Is learning better networks as easy as stacking more layers?</font>**

그러나 단순히 more layer에는 다음과 같은 문제를 야기한다.



- Vanishing/exploding gradient



위의 문제(layer가 깊어지면서 역전파로 얻는 gradient가 너무 작아지거나 커지면서 생기는 현상)은 normalized initialization(가중치를 초기에 적절히 초기화), intermediate normalization layer의 방법을 통해 개선하여 수십개의 layer까지는 해결하였다.

그러나 더 깊어질수록, degradation 문제가 발생한다.

***degradation 이란?***

- **degradation** : 모델의 capacity가 너무 증가하면서 **train accuracy(test도 마찬가지)**가 낮아짐

- **overfitting** : 모델의 capacity가 증가하면서 **test accuracy**가 낮아짐

이 degradation의 문제는 단순히 오버피팅때문만이 아니라 layer가 많아질 수록 생기는 문제이다. 

![image-20240130205449608](/images/2024-01-30-ResNet/image-20240130205449608.png)

위의 그래프가 그 예시인데, 그림에서 볼 수 있듯이 layer가 더 깊은 layer(56-layer)가 training error, test error 모두 높은 것을 확인할 수 있다.



## **Core**



**<font color = 'red'>motive idea</font>** : **deeper model should produce no higher training error than its shallower counter part**

**얇은 모델에 identity mapping layer를 추가하면, training error가 높아지진 않아야한다.**



> identity mapping layer이란?
>
> - 단순히 아무것도 하지 않는 layer
> - convolution을 통과하지 않고 값을 전달
> - 입력값을 그대로 전달한다는 의미에서 identity



이를 해결하기 위해 해당 논문에서는 **<font color= 'red'>deep residual learning framework </font>**를 제안한다. 



## **What is Residual Learning**



기존 네트워크는 input $x$를 받고, layer를 거쳐 $H(x)$를 output으로 출력하는데, 이는 input 값 $x$를 타겟값 $y$로 mapping하는 함수$H(x)$를 얻는 것이 목적이다.

이때 ResNet의 Residual Learning은 $H(x)$를 직접 학습시키는 것이 아니라, 출력과 입력의 차(잔차)인  $H(x)-x$를 얻도록 목표를 수정한다. 

수학적으로 보면 $H(x)$를 학습시키는 것과 별다른 차이가 없는 것으로 보일 수 있는데 $F(x)=H(x)-x$를 학습시키는 것이 훨씬 쉽다.

그 이유는, 이전에는 Unreferenced mapping인 $H(x)$를 학습시켜야 하기에 어려움이 있었지만(처음보는 $H(x)$를 최적의 값으로 찾는 과정), 이전 **layer의 값인 $x$를 보존하고, 추가적으로 필요한 정보를 학습하기때문에 학습하기가 더 쉽다.**

**그것이 아니라면 함수는 매번 새로운 mapping을 진행하기때문에 학습하기가 더 어렵다.**

**또한, $F(x)$가 vanishing gradient 현상으로 학습이 되지않아 zero mapping이 되어도, 최종 output인 $H(x)$는 $x$가 남아 identity mapping이 가능하다.**

**--> 위의 motivate idea가 여기서 결정적인 역할을 하는데, <font color = 'red'>vanishing gradient의 문제가 발생하여도, 성능 저하가 일어나지 않는다. 성능 저하가 일어나지 않기때문에 더 많은 layer를 쌓을 수 있고 결론적으로 더 깊은 layer를 쌓을 수 있다.</font>**

> 무슨 말??
>
> Identity mapping이 optimal할 확률은 매우 희박하지만, 그러한 mapping을 추가하는 것 자체가 문제를 더 쉽게 해결할 수 있도록 하는 좋은 방향성을 제시해줄 수 있다.
>
> identity mapping이 optimal에 가깝다면, 학습하기 더 쉬울 것이다.



## **What is shortcut connection**

**Skip connection**이라고도 한다.

네트워크의 입력을 하나 이상의 레이어를 건너뛰어 출력에 직접 더하는 구조이다.

해당 논문에서는, output layer에 identity mapping하는 것을 말한다.

**building block** : $y=F(x,{W_{i}})+x$

$y$가 output이고, $x$가 input이다. 

$F(x, {W_{i}})$는 residual mapping을 의미하고, $x$는 identity mapping(shortcut mapping)을 의미한다. 

위의 shortcut connection을 하는 것은 추가적인 파라미터가 필요하지 않고, 복잡도가 증가하지않는다

또한 SGD에 따른 역전파 end-to-end 학습이 가능하다.

이때의 $x$, $F$의 dimension은 같아야한다.

$F = W_{2}\sigma(W_{1}x)$

여기서 $\sigma$는 ReLU계산을 뜻하고, bias는 고려하지않는다.

dimension이 같지않을 때, 아래의 식처럼 $W_{s}$ 을 linear projection을 통해 dimension을 맞춰준다._ 

_$y=F(x,{W_{i}})+W_{s}x$

<img src="https://velog.velcdn.com/images%2Fqsdcfd%2Fpost%2F974ed89f-0249-412d-a772-86c3355f8d87%2Fimage.png" alt="img" style="zoom:50%;" />

위 처럼, input 값과 residual mapping한 값의 dimension이 같지 않다면, 아래의 그림처럼 projection을 통해 dimension을 맞춰준다.

<img src="https://velog.velcdn.com/images%2Fqsdcfd%2Fpost%2Fcfc19af4-3456-44cd-823d-298e941a378d%2Fimage.png" alt="img" style="zoom:50%;" />



## **Compare Architectures Plain Net vs ResNet**



<img src="/images/2024-01-30-ResNet/image-20240130224716228.png" alt="image-20240130224716228" style="zoom:50%;" />

> Example network architectures for ImageNet. Left: the VGG-19 model [40] (19.6 billion FLOPs) as a reference. Middle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 shows more details and other variants.

위는 VGG-net과 VGG-net과 유사한 34-layer net, 34-residual net을 비교한 그림이다.

34-layer net과 34-residual net의 차이점은 오직 residual function을 추가한 것이다. 

이때 점선은 입력단과 출력단의 dimension이 일치하지 않아, dimension을 맞추는 shortcut connection이 이용된 부분이다.

비교해보면 VGG와 비교했을 때 계산 복잡도의 지표인 FLOPs는 감소한 것을 확인할 수 있다.(=VGG보다 연산량이 감소했다.)

입력단과 출력단의 dimension이 동일할때는 identity mapping 사용할 수 있다.

dimension이 다르면, (A) 한 쪽에 padding을 하고  identity mapping하는 방법과 (B) projection 연산을 활용한 shortcut connection 방법을 이용한다.



## **Experiment & Result**

<img src="/images/2024-01-30-ResNet/image-20240130225003657.png" alt="image-20240130225003657" style="zoom:50%;" />

위는 (ImageNet에 적용한)ResNet의 layer 수에 따른 architecture을 보인다.



**Plain network**

<img src="/images/2024-01-30-ResNet/image-20240130231829469.png" alt="image-20240130231829469" style="zoom:50%;" />

<img src="/images/2024-01-30-ResNet/image-20240130231848668.png" alt="image-20240130231848668" style="zoom: 50%;" />

plain network는 layer가 깊어질수록, 성능이 안 좋아지고, ResNet은 layer가 깊어질 수록 성능이 좋아지는 것을 확인 할 수 있다.

필자는 plain network의 이러한 문제가 vanishing gradient 때문이라고 하지 않는다.

학습 과정에서 forward, backward signal이 사라지는 문제는 발견되지 않았으며, 이러한 문제는 **exponentially low convergence rate** (극단적으로 낮은 수렴율)때문이라고  필자는 추정한다.

---

**ResNet**



**Identity vs Projection**

(A) zero-padding을 통해 dimension을 증가시키고, identity mapping

(B) dimension이 증가할 때 마다 projection 연산

(C) 모든 shortcut에 대해서 항상 projection 연산

<img src="/images/2024-01-30-ResNet/image-20240130232728781.png" alt="image-20240130232728781" style="zoom: 50%;" />



성능을 확인해보면, 성능 자체는 (C)가 가장 성능이 좋은 것으로 확인이 된다. 

다만 본 논문에서는 projection shortcut이 필수라고 할 정도로 큰 차이라고 보지 않는다. 

기본적으로 identity shortcut을 이용해 성능을 많이 개선시킬 수 있기때문에, 해당 논문에서는 메모리, 시간 복잡도, 모델 size를 줄이기 위해 (C)를 이용하지않는다.

---

### **Bottleneck architecture**



<img src="/images/2024-01-30-ResNet/image-20240130232923981.png" alt="image-20240130232923981" style="zoom:67%;" />

오른쪽 그림(Bottleneck architecture)을 통해 확인하면, 초반에 1x1, 64를 사용하고 중간에는 3x3, 64 filter를 사용하고 마지막에는 1x1, 256을 이용하는 것을 확인할 수 있다.

(위와 같이 중간 볼록한 구조를 Bottleneck Architecture이라고 한다.)

1x1 Layer는 Dimension을 줄였다가 키우는 역할을 한다

이유는 3x3 Layer의 Input과 Output Dimension을 작게 만들기 위해서이다. 

파라미터 측면에서 봤을 때, 작은 커널을 사용함으로써 파라미터의 수가 줄어든다고 이해할 수 있다.



위의 Bottleneck 구조의 예시를 보면, Layer가 1개 더 많음에도 불구하고 Time Complexity는 둘이 비슷하다.

여기서 **Identity Shortcuts( = identity mapping by shortcut, Identity mapping by shortcut)은 중요한 역할을 한다.** 만약에 Identity Shortcut이 Projection으로 바뀐다면 Time Complexity와 모델의 크기는 두배가 된다.

**따라서 Identity Shortcut은 Bottleneck 구조에서 효율적인 구조를 위해 꼭 필요하다.**



**50 - layer ResNet**

기존에 만들었던 34-layer에 3-layer bottleneck block을 추가해서 50-layer ResNet을 만든다.(option (B) dimension이 증가할 때 마다 projection 연산)

깊이가 50 layer ResNet을 34 layer ResNet와 비교했을 때, 파라미터와 계산속도는 거의 비슷한 것을 확인할 수 있다.



**101-layer and 152-layer ResNet**

마찬가지로, 101 layer ResNet과 152 layer ResNet은 depth가 상당히 증가했음에도 여전히 낮은 복잡성을 가진다.

layer를 깊게 쌓으면 더욱 좋은 성능을 확인할 수 있는 것을 확인 할 수 있고, degradation problem이 관찰되지 않는다.



ensemble을 통해 가장 좋은 성능(3.57%)을 보일 수 있다.



### **Layer Response**

![image-20240131030035060](/images/2024-01-30-ResNet/image-20240131030035060.png)





ResNet에 적용하였을 때, ResNet은 일반적으로 Plain Net보다 작은 Response를 보였다. 

이것을 통해 앞서 얘기했던 non-residual function보다 residual function이 0에 더 가깝다는 것을 말해준다. 

즉, Identity Mapping이 되도록 Weight가 0이 되도록 학습이 되었다는 것이다. 

그리고 깊은 모델일수록 Response의 크기가 작았다. 

Layer가 증가할수록 신호의 변화가 줄어드는 경향을 보인다.



### **Exploring Over 1000 Layers**

> 그럼 layer를 계속 늘리면 어떨까?

위 질문에 대한 답으로, 1202개의 Layer를 가진 모델을 실험하였고, Optimization 과정의 문제는 없었다. 

![image-20240131030328367](/images/2024-01-30-ResNet/image-20240131030328367.png)

위의 Table을 보면 꽤 괜찮은 성능을 보이고 있다. 

하지만 여기에도 문제들이 있었다. Training Error는 두 모델이 비슷했지만, 1202 Layer의 모델이 110 Layer 모델보다 성능이 좋지 않았다. 그 이유를 여기서는 Overfitting을 이야기한다. 

1202 모델은 ImageNet을 학습하기에는 너무나도 큰 모델이다. 

(여기서 말하는 큰 모델은 파라미터 수가 많다는 것이다. )



## **Conclusion**



결론적으로 shortcut connection은 덧셈 연산량이 증가할 뿐, 학습 파라미터의 수에는 큰 영향이 없으며, 더 많은 수의 레이어를 갖는 깊은 모델도 잘 학습하고, 심지어 더 쉽고 빠르게 학습이 가능하다는 것이다.

ResNet은 현재도 많은 모델들의 대표적인 backbone으로 사용되고 있으며, 발전된 형태의 inception-resnet이나 resnext등도 많이 공개되어있다. 



## 



---

#### _<u>Reference</u>_

- [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
- [ResNet review video](https://www.youtube.com/watch?v=671BsKl8d0E&t=2421s)
- [[Classic] Deep Residual Learning for Image Recognition (Paper Explained)](https://www.youtube.com/watch?v=GWt6Fu05voI&t=96s)
- [pesudo-lab: PyTorch](https://pseudo-lab.github.io/pytorch-guide/docs/ch03-1.html)
- 



















