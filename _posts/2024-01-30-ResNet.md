---
layout: single
title: "[Paper Review] Deep Residual Learning for Image Recognition"
categories: [AI, Paper Review]
tag: []
typora-root-url: ../
use_math: true
---

<font color=gray>ResNet을 제시하여 이미지 인식에 큰 영향을 끼친 'Deep Residual Learning for Image Recognition' 논문 리뷰입니다.</font> <br>

> 논문 list는 이 [repo](https://github.com/SEUNGW00LEE/paper_review)에 정리되어있습니다.

# [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

---

**K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, Las Vegas, NV, USA, 2016, pp.**

---

**본 논문은 매우 좋은 논문이라고 하는 것에는 크게 2가지 이유가 있다.**

1. **ResNet의 성능이 매우 좋다.**

2. **논문에서 제안한 아이디어가 이해하기 쉽다.**



 **Main Idea : 깊은 네트워크를 학습시키기 위한 방법으로 residual learning을 제안한다.**



## **0. Overview**

**지금까지의 어떤 문제가 있었나?**

- depth가 깊이가 모델의 성능에 큰 영향을 끼치는 것은 자명하지만, 이전의 framework는 network가 깊어질수록 훈련시키기 어려운 문제를 가진다. (오버피팅, gradient 소멸, 연산량 증가)

**해당 논문에서는 어떻게 해결할 수 있나?**

- 함수를 새로 만드는 방법 대신에 residual function(잔차 함수)을 통해 잔차 함수를 learning에 사용하는 것으로 layer를 재구성한다. 
- Residual이란 결과의 오류라고 할 수 있는데, 지금까지 평가의 기준으로 삼았지만 이를 이용해 학습하였다.
- 이를 통한 학습은 학습하기 더 쉽고 이전의 모델보다 훨씬 깊은 layer(152 layer)를 가질 수 있다.

**해당 연구를 통한 성과는 무엇이 있나?**

1. ResNet은 VGG net보다 8배 깊은 152 layer를 가지지만 복잡하지 않고 최적화시키기 쉽고, 깊은 깊이에서도 정확도를 얻을 수 있다.
2. 여기에 앙상블 기법을 적용하여 3.57% error를 가진다.
3. Image classification 뿐만 아니라, object detection등 전분야에서 탁월한 성능을 보인다.



## **1. Introduction**

지금까지의 연구를 통해 깊은 ConvNet은 이미지 분류에 있어 중요한 요소이며, 좋은 결과를 위해서는 많은 layer가 필요하다는 것을 알 수 있다.

이전의 VGG-Net이나, GoogLeNet 또한 layer가 깊어짐에 따라 얻는 장점을 적절히 취했다.

필자는 아래와 같은 질문을 남긴다.

> Is learning better networks as easy as stacking more layers?

그러나 단순히 more layer에는 다음과 같은 문제를 야기한다.

- Vanishing/exploding gradient

위의 문제는 normalized initialization(가중치를 초기에 적절히 초기화), intermediate normalization layer의 방법을 통해 개선하여 수십개의 layer까지는 해결하였다.

그러나 더 깊어질수록, degradation 문제가 발생한다.

이 degradation의 문제는 단순히 오버피팅때문만이 아니라 layer가 많아질 수록 생기는 문제이다. 

![image-20240130205449608](/images/2024-01-30-ResNet/image-20240130205449608.png)

위의 그래프가 그 예시인데, 그림에서 볼 수 있듯이 layer가 더 깊은 layer(56-layer)가 training error, test error 모두 높은 것을 확인할 수 있다.



이를 해결하기 위해 해당 논문에서는 "deep residual learning framework" 를 제안한다. 

![image-20240130221654000](/images/2024-01-30-ResNet/image-20240130221654000.png)

residual function이란 $H(x)$를 직접 학습시키는 것이 아니라, 학습하기 더 쉬운 $F(x) = H(x)-x$를 통해 학습시키는 것을 말한다. 이때, 실질적인 output은 $H(x) + x$가 된다. 

**short connection** : skip connection이라고도 하는데, 이는 $H(x) + x$을 말하며, 별도의 파라미터가 필요하지않고 추가적인 복잡도가 증가하지 않는다는 점이 장점이다.

또한 구현조차 간단하다.

이러한 residual net은 깊이가 깊어지면 깊어질수록 높은 accuracy를 가진다.

특정 데이터 셋에 국한되지 않는다.

object detection, segementation detection에도 좋은 성능을 보였다.



$F(x)$를 학습시키는 것이, 원래 학습시키는 $H(x)$보다 쉽다는 것을 보인다.



## **2. Related Work**

Residual Representations : 본 논문에서뿐만 아니라, 이전부터 자주 사용된 것을 보인다.

Shortcut connection : 딥러닝 연구에 많이 사용이 됐다.



## **3. Deep Residual Learning**

### **3.1 Residual Learning**

$H(x)$을 의도했던 optimal한 mapping이라고 할 때, neural network가 하는 일은 여러개의 non-linear한 layer를 이용해 점진적으로 복잡한 함수를 학습하는 것이라고 할 수 있다.

Residual Learning에서는 $H(x) - x$를 학습시키는 것이라고 할 수 있는데, 수학적으로 보면 $H(x)$를 학습시키는 것과 별다른 차이가 없는 것으로 보일 수 있는데 $F(x)=H(x)-x$를 학습시키는 것이 훨씬 쉽다는 것이 중요한 개념이다.

이것은 오히려 직관에 반하는 현상인데, 추가적으로 더해지는 layer가 identical mapping이라고 한다면, 더 깊은 모델이 적어도 얕은 모델보다 training error가 크지는 않을 것이다.

입력값을 항상 출력값으로 더해질 수 있게 함으로써 identity mapping을 항상 수행할수있도록 하여 학습 난이도를 낮추는 것이다.

Identity mapping이 optimal할 확률은 매우 희박하지만, 그러한 mapping을 추가하는 것 자체가 문제를 더 쉽게 해결할 수 있도록 하는 좋은 방향성을 제시해줄 수 있다.

identity mapping이 optimal에 가깝다면, 학습하기 더 쉬울 것이다.

이전 layer의 값인 $x$를 보존하고, 추가적으로 필요한 정보를 학습하기때문에 학습하기가 더 쉽다.

그것이 아니라면 함수는 매번 새로운 mapping을 진행하기때문에 학습하기가 더 어려울 수 있다.

### **3.2 Identity Mapping by Shortcuts**

$y= F(x, {W_{i}}) + x$

$F(x, {W_{i}})$는 residual mapping을 의미하고, $x$는 identity mapping(shortcut mapping)을 의미한다.



<img src="/images/2024-01-30-ResNet/image-20240130212437063.png" alt="image-20240130212437063" style="zoom:50%;" />

이를 도식화하면 위와 같다.

이 식에서 bias 값은 고려하지않도록 정의가 됐다.

Plain Network (VGG-Net)

![image-20240130224716228](/images/2024-01-30-ResNet/image-20240130224716228.png)

> Example network architectures for ImageNet. Left: the VGG-19 model [40] (19.6 billion FLOPs) as a reference. Middle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 shows more details and other variants.

VGG-net과 VGG-net과 유사한 34-layer net, 34-residual net을 비교한 그림이다.

34-layer net과 34-residual net은 오직 residual function을 추가한 것이다. 

이때 점선은 입력단과 출력단의 dimension이 일치하지 않아, dimension을 맞추는 shortcut connection이 이용된 부분이다.

비교해보면 VGG와 비교했을 때 계산 복잡도의 지표인 FLOPs는 감소한 것을 확인할 수 있다.

입력단과 출력단의 dimension이 동일할때는 identity mapping 사용할 수 있다고 한다.

그렇지 않다면, 한 쪽에 padding을 하고 identity mapping하는 것과 projection 연산을 활용한 shortcut connection이다. 

![image-20240130225003657](/images/2024-01-30-ResNet/image-20240130225003657.png)

위 그림의 34-layer은 위의 34-resNet이다. 



### **3. Implementation**













