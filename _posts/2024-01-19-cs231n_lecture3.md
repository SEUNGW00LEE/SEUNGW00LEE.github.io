---
layout: single
title: "[CS231n review] Lecture3 - Loss function & Optimization"
categories: [AI, Computer Vision, CS231n]
tag: [CS231n, Computer Vision, AI, Image Classification]
typora-root-url: ../
use_math: true

---

<br><font color=gray>Stanford CS231n(2017)를 학습하며 정리 및 추가한 내용입니다.</font> <br>

<br>



## Loss function & Optimization



<img src="/images/2024-01-19-cs231n_lecture3/visualizing-the-loss-landscape-of-neural.png" alt="visualizing-the-loss-landscape-of-neural" style="zoom: 33%;" />



Lecture 2에서 배운 바와 같이, 학습한다는 것은 좋은 $ W(parameter) $ 를 찾는 것입니다. 해당 강의에서는 좋은 W를 찾는 과정에 대해 학습했다.

<br>

좋은 W를 찾기 전에 가장 먼저 정해야 할 것이 있다.

> 무엇이 좋은 것인가?

무엇이 좋은 W인지 **<u>판단</u>**하기 위해 **$Loss function$**을 정해야한다. <br>



<br>

강의에서, Loss function을 기준으로 SVM classifier와 softmax classifier를 설명한다.

### 1. SVM Classifier

$ x_{i}$ 는 이미지이고, $y_{i}$ 는 정답 라벨링일때, $s =f(x_{i},W)$를 통해 s를 구한다.

SVM은 Hinge Loss라고도 불리며 아래와 같이 구성된다.

![스크린샷 2024-01-19 오후 5.12.46](/images/2024-01-19-cs231n_lecture3/스크린샷 2024-01-19 오후 5.12.46.png)
$$
L_i = \begin{cases} 0 & \text{if} \,\, s_{y_{i}} \ge s_{j} + 1, \\ s_{j}-s_{y_{i}} + 1 & \text{otherwise} \end{cases}
$$


