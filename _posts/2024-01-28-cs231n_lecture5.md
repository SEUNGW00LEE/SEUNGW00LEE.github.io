---
layout: single
title: "[CS231n review] Lecture5 - Convolutional Neural Networks"
categories: [AI, Computer Vision, CS231n]
tag: [CS231n, Computer Vision, AI, CNNs]
typora-root-url: ../
use_math: true

---

<br><font color=gray>Stanford CS231n(2017)를 학습하며 정리 및 추가한 내용입니다.</font> <br>

<br>

![image-20240128170200481](/images/2024-01-28-cs231n_lecture5/image-20240128170200481.png)

## Convolutional Neural Networks

---

> Convolve the filter with the image i.e. "slide over the image spatially computing dot product"

**convolution ?** : **하나의 함수와 또 다른 함수를 반전 이동한 값을 곱한 다음, 구간에 대해 적분하여 새로운 함수를 구하는 수학 연산자**

![image-20240128170731334](/images/2024-01-28-cs231n_lecture5/image-20240128170731334.png)

image 와 filter의 depth 는 같고, 한 filter는 하나의 activation map을 생성한다.

![image-20240128170825542](/images/2024-01-28-cs231n_lecture5/image-20240128170825542.png)

두번째 filter는 또다른 activation map을 생성한다.

마찬가지로, n개의 필터로 n개의 activation map을 생성한다.

위의 예시에서는 $28 \times 28 \times n$을 re-representation한다.

 ![image-20240128171019205](/images/2024-01-28-cs231n_lecture5/image-20240128171019205.png)

이를 연속적으로 한다면 위와 같이 형성된다.

6개의 $5\times5\times3$의 필터로 $28 \times 28 \times 1$ 사이즈의 $28 \times 28\times 6$ activation maps를 형성할 수 있고, 이를 다시 10개의 $5\times5\times6$ 필터로 $24\times24\times10$ activation map를 생성할 수 있다.



위처럼 Convolution Layer를 여러개 이어 붙혀 사용할 수 있는데, 이 **Convolutional Layer가 깊어질수록 더 복잡하고, 정교한 특징을 얻는다.**



update해야할 parameter의 대상은 filter이다.



![image-20240128172255879](/images/2024-01-28-cs231n_lecture5/image-20240128172255879.png)

위처럼, CONV, ReLU, Pool를 반복적으로 진행하고, 마지막에 FC layer로 class의 score을 계산합니다.

### **Convolution Layer Strider**

> Stride : filter가 image를 슬라이딩할 때 움직이는 step의 크기

<img src="https://velog.velcdn.com/images%2Ffbdp1202%2Fpost%2F2c8ec5f6-a20f-4dd9-9fae-9aded7ac8fa0%2Fcs231n-05-010-convolutional_layer_cal_ex.gif" alt="img" style="zoom: 33%;" />

위는 stride 크기가 1인 경우이고

아래는 stride 크기가 2인 경우를 보여준다.

![img](https://velog.velcdn.com/images%2Ffbdp1202%2Fpost%2Ff5478fa9-4b89-47a8-bcdb-c9149898a202%2Fcs231n-05-011-convolutional_layer_cal_stride.gif)

Output의 size를 공식화 할 수 있는데 $(N(image\ width) - F(filter\ width)) / stride + 1$이다.



stride가 input image에 맞지않으면 불균형한 결과를 부를 수 있다.

예를 들어, stride가 3이고, input image size가 7x7이고, filter size가 3x3일 때, output size는 2.33이 나오게 된다.



### **Convolutional Layer Pad**

위에서 본 것과 같이 Convolutional Layer은 Filter 크기에 따라 width와 height가 줄어드는데, 만약 Convolution Layer가 여러 층이라면 image의 크기가 점점 줄어드는 문제가 있다.

**Convolutional Layer Pad는 원본 크기를 유지하며, image의 외곽이 덜 계산되는 것을 방지한다.**

0으로 채운다면 **Zero-padding**이라고 한다.

![img](https://velog.velcdn.com/images%2Ffbdp1202%2Fpost%2F7260f062-912f-4047-9088-b2a238df633c%2Fcs231n-05-012-convolutional_layer_padding.gif)

### **Convolution Layer Output Size**

- `W` : input image width
- `F` : Filter width
- `S` : Stride
- `P` : Pad
- `Output W` : `(W - F + 2*P)/S + 1`



**Example** : Input volume : 32x 32x 3, 10 5x5x3 filter with stride1 , pad 2

- --> Output volume size : (32+2x2-5)/1 + 1=32, so 32x32x10
- --> Number of parameters in this layer? 

각각의 filter에는 5x5x3의 75개와 bias까지해서 76개의 parameter가 존재한다. 

필터가 총 10개이므로 parameter는 총 760개가 존재한다.



### **1x1 Convolution Layer**

![image-20240128180649261](/images/2024-01-28-cs231n_lecture5/image-20240128180649261.png)

1x1 convolution 도 의미가 있는데, 그 이유는 3차원이기 때문이다.





### **Convolution Layer의 특징**

Activation map

1. Each is connected to small region in the input
2. All of them share parameters (parameter sharing)

![image-20240128181312157](/images/2024-01-28-cs231n_lecture5/image-20240128181312157.png)

위와 같이 activation maps의 같은 위치에 있는 뉴런들은 input image의 동일한 위치를 바라보고, 각각 다른 weight를 가진다.(weight를 공유하지 않는다.) <-> 동일한 depth : parameter sharing 



### **Pooling layer**

- make the representations smaller and more manageable
- operates over each activation map independently

![image-20240128181554741](/images/2024-01-28-cs231n_lecture5/image-20240128181554741.png)

activation map의 크기를 downsampling 하는 과정(이미지의 크기를 줄이는 과정)

convolution layer의 깊이가 깊어질수록 많은 계산 양을 요구하므로, pooling layer를 통해 이미지의 크기를 줄여 속도를 높일 수 있다.



#### **Max pooling**

![image-20240128181738791](/images/2024-01-28-cs231n_lecture5/image-20240128181738791.png)

가장 큰 값을 추출하는 방식이다.

해상도를 2배로 줄일 때 가장 잘 보이는 성분을 남기는 것과 같다.

(약간의 정보를 손실하면서 invariance한 능력을 가진다.)



### **Fully Connected Layer(FC layer)**

Cone layer의 출력은 3차원 volume으로 이뤄지는데, 이를 전부 펴서 1차원 벡터로 만들어 FC Layer의 입력으로 사용한다.

이는 Cons net의 모든 출력을 서로 연결하게 되는 것이다.(마지막 layer에서는 공간적 구조를 신경쓰지 않아도 된다.)

이를 통해 최종적으로 각 class 별로 score가 출력으로 나온다.



### **Case Study**

- **LeNet-5**

![image-20240128185539449](/images/2024-01-28-cs231n_lecture5/image-20240128185539449.png)

C : convolution, S : subsampling(fooling)

- **AlexNet**

![image-20240128185716028](/images/2024-01-28-cs231n_lecture5/image-20240128185716028.png)

두개의 별도의 stream을 이용하는데, 그 이유는 그 당시의 GPU의 성능을 최대화 하기위해(최근에는 그냥 하나로 돌림)

> Output volume : 55x55x96
>
> Parameter : (11x11x3)x96 = 35K
>
> Second layer: (Pool1) : 3x3 filter applied at stride 2
>
> Q. What is the output volume size? 
>
> A. (55-3)/2 + 1 = 27 --> 27x27x96
>
> parameter : 0 !(pooling layer에는 parameter가 없다.)

결국 계산하면 아래와 같은데, 이때 Normalization layer은 현재에는 거의 사용하지 않는다.

FC7 layer는 classifier 직전에 있는 layer를 일반적으로 칭한다.

![image-20240128191946323](/images/2024-01-28-cs231n_lecture5/image-20240128191946323.png)

- AlexNet 특징
  - AlexNet에서 처음으로 ReLU를 사용했다.
  - Norm layer를 사용
  - Data augmentation을 많이 사용

- **ZFNet**

![image-20240128192253800](/images/2024-01-28-cs231n_lecture5/image-20240128192253800.png)

AlexNet과 거의 유사하나, 

CONV1 : filter size를 변경(줄임), CONV3,4,5의 filter 수를 늘림

- **VGGNet**

![image-20240128192819546](/images/2024-01-28-cs231n_lecture5/image-20240128192819546.png)

오직 CONV stride1, pad1 & 2x2 MAX POOL stride2 만 이용한다.

![image-20240128193003002](/images/2024-01-28-cs231n_lecture5/image-20240128193003002.png)

image size는 줄지만, filter의 수는 늘어나는 것을 볼 수 있다.

한 image당 약 200mb의 메모리를 사용한다.

parameter은 1억3800만

memory 사용량을 보면 대부분 앞에서 할당되고, parameter은 뒤에 할당된다.

때문에 최근에는 FC layer를 사용하는 것이 비효율적이고, 대신에 average pooling을 이용하는 연구가 진행된다.

![image-20240128193515522](/images/2024-01-28-cs231n_lecture5/image-20240128193515522.png)

Inception module이 연속적으로 이어지고, avg pool을 이용한다.

avg pool을 이용하여 parameter을 상당히 줄여, AlexNet보다 12배 적은 파라미터를 가지고, 2배의 속도를 가지게 된다.

상당히 복잡한 모델이기때문에 1등을 했음에도 불구하고 2등이지만 간단한 VGGNet이 대중적으로 이용됐다.

- **ResNet**

![image-20240128193848300](/images/2024-01-28-cs231n_lecture5/image-20240128193848300.png)

**ImageNet Classification뿐만 아니라 모든 분야에서 석권을 하였다.**

![image-20240128193928744](/images/2024-01-28-cs231n_lecture5/image-20240128193928744.png)

![image-20240128194044874](/images/2024-01-28-cs231n_lecture5/image-20240128194044874.png)

ResNet의 주장은 지금까지의 연구(plain net)는 layer가 많아짐에 따라 error rate가 증가하지만, ResNet의 경우 layer가 증가함에 따라 error rate가 감소한다.

![image-20240128194157796](/images/2024-01-28-cs231n_lecture5/image-20240128194157796.png)

ResNet의 가장 큰 특징은 layer가 상당히 증가한 것이다. 

따라서 training 기간은 상당히 오래 걸리지만, test 시에는 상당히 빠른 결과를 냈다.(VGG보다 8배 이상 빠른 결과)

![image-20240128194341989](/images/2024-01-28-cs231n_lecture5/image-20240128194341989.png)

ResNet에서 주목해야하는 것 CONV layer를 한번 거친 후 바로 pooling이 들어가는데, 이후 layer에서 쭉 들어간다.

초기에 size를 줄이기때문에 매우 효율적이고, 이후 skip connection을 통해 효율적 연산이 이뤄진다.

![image-20240128194627928](/images/2024-01-28-cs231n_lecture5/image-20240128194627928.png)

ResNet같은 경우에는 skip connection 이 있고 + 연산이 있는데,

backpropagation을 할 때 바로 이전의 CONV로(126개의 layer를 한번에 넘어가게) 갈 수 있다.

- 모든 CONV layer 이후 Batch Normalization을 사용한다.
- Xavier/2를 이용한다.
- AlexNet(0.01)보다 큰 learning rate(0.1)을 이용한다.
  - Error 가 정체되면 10으로 나눈다.













---

#### *<u>Reference</u>*

- [Stanford CS231n](http://cs231n.stanford.edu/)
- [DSBA Lap semina](http://dsba.korea.ac.kr/seminar/?mod=document&uid=17)
- [A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)
- [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)
- [[CS231n 정리] 5. Convolutional Neural Networks](https://velog.io/@fbdp1202/CS231n-%EC%A0%95%EB%A6%AC-5.-Convolutional-Neural-Networks)
- ## 
